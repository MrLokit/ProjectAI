import speech_recognition as sr
import pyttsx3
import datetime
import numpy as np
import json
import random
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import pickle
import re
import os


class UltraMegaJarvis:
    def __init__(self):
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()
        self.tts_engine = pyttsx3.init()

        # –£–õ–¨–¢–†–ê –ú–ï–ì–ê –î–ê–ù–ù–´–ï - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
        self.intents = self.create_mega_dataset()

        self.vectorizer = None
        self.label_encoder = None
        self.model = None
        self.train_ultra_mega_network()

        print("–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞...")
        with self.microphone as source:
            self.recognizer.adjust_for_ambient_noise(source, duration=3)
        print("–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

    def create_mega_dataset(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—å—Ç—Ä–∞-–º–µ–≥–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        return {
            "greeting": {
                "patterns": self.generate_variations([
                    "–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", "—Ö–∞–π", "hello",
                    "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ", "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä", "–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é", "—Å–∞–ª—é—Ç",
                    "–ø—Ä–∏–≤–µ—Ç –¥—Ä—É–≥", "–ø—Ä–∏–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ", "–ø—Ä–∏–≤–µ—Ç–∏–∫",
                    "–¥–æ–±—Ä–æ–π –Ω–æ—á–∏", "–ø—Ä–∏–≤–µ—Ç –≤—Å–µ–º", "–∑–¥–æ—Ä–æ–≤–æ", "–º–æ–µ –ø–æ—á—Ç–µ–Ω–∏–µ",
                    "–¥–æ–±—Ä–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫", "–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤–∞—Å", "—Ä–∞–¥ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å",
                    "–ø—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞", "–ø—Ä–∏–≤–µ—Ç —Å—Ç–∞—Ä—à–∏–π", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π –¥–∂–∞—Ä–≤–∏—Å",
                    "–ø—Ä–∏–≤–µ—Ç —É–º–Ω–∏–∫", "–¥–æ–±—Ä—ã–π", "–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ", "–∑–¥–∞—Ä–æ–≤–∞", "—Ö–∞–π —Ç–∞–º",
                    "–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é —Ç–µ–±—è", "–¥–æ–±—Ä–æ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è", "–ø—Ä–∏–≤–µ—Ç–∏–∫–∏", "–∑–¥–æ—Ä–æ–≤–µ–Ω—å–∫–æ",
                    "–¥–æ–±—Ä–æ–≥–æ –¥–Ω—è", "–ø—Ä–∏–≤–µ—Ç–∏–∫ –≤—Å–µ–º", "–∑–¥–∞—Ä–æ–≤", "–ø—Ä–∏–≤–µ—Ç –Ω–∞—Ä–æ–¥",
                    "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä–æ–∫", "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–µ—á–∫–æ", "–ø—Ä–∏–≤–µ—Ç–∏–∫–∏ –ø–∏—Å—Ç–æ–ª–µ—Ç–∏–∫–∏"
                ], 50),
                "responses": [
                    "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!", "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?", "–†–∞–¥ –≤–∞—Å —Å–ª—ã—à–∞—Ç—å!", "–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é!",
                    "–ó–¥–æ—Ä–æ–≤–æ!", "–ü—Ä–∏–≤–µ—Ç! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?", "–î–æ–±—Ä—ã–π –¥–µ–Ω—å!", "–ü—Ä–∏–≤–µ—Ç, –¥—Ä—É–≥!",
                    "–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤–∞—Å!", "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π! –ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ?"
                ]
            },
            "time": {
                "patterns": self.generate_variations([
                    "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å", "—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º—è", "–≤—Ä–µ–º—è", "time", "current time",
                    "—Å–∫–∞–∂–∏ –≤—Ä–µ–º—è", "–ø–æ–¥—Å–∫–∞–∂–∏ –≤—Ä–µ–º—è", "—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏", "–∫–æ—Ç–æ—Ä—ã–π —Å–µ–π—á–∞—Å —á–∞—Å",
                    "–≤—Ä–µ–º—è —Å–µ–π—á–∞—Å", "—Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è", "—Å–∫–æ–ª—å–∫–æ —Å–µ–π—á–∞—Å –≤—Ä–µ–º–µ–Ω–∏", "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å –≤—Ä–µ–º–µ–Ω–∏",
                    "–ø–æ–¥—Å–∫–∞–∂–∏ –∫–æ—Ç–æ—Ä—ã–π —á–∞—Å", "—Å–∫–∞–∂–∏ –∫–æ—Ç–æ—Ä—ã–π —á–∞—Å", "–≤—Ä–µ–º—è —É–∑–Ω–∞—Ç—å", "—É–∑–Ω–∞—Ç—å –≤—Ä–µ–º—è",
                    "—Å–∫–æ–ª—å–∫–æ –Ω–∞ —á–∞—Å–∞—Ö", "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å –Ω–∞ —á–∞—Å–∞—Ö", "–≤—Ä–µ–º—è —Å—É—Ç–æ–∫", "—Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è",
                    "–ø–æ–¥—Å–∫–∞–∂–∏ —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è", "—Å–∫–∞–∂–∏ —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è", "–≤—Ä–µ–º—è —Å–µ–π—á–∞—Å —Å–∫–æ–ª—å–∫–æ",
                    "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å –ø–æ–¥—Å–∫–∞–∂–∏", "—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–µ–π—á–∞—Å", "–≤—Ä–µ–º—è —É–∑–Ω–∞—Ç—å —Å–µ–π—á–∞—Å",
                    "—Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è —É–∑–Ω–∞—Ç—å", "–≤—Ä–µ–º—è –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç", "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å –≤—Ä–µ–º–µ–Ω–∏ —Å–µ–π—á–∞—Å"
                ], 40),
                "responses": ["–°–µ–π—á–∞—Å {}", "–¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {}", "–ù–∞ —á–∞—Å–∞—Ö {}", "–í—Ä–µ–º—è: {}"]
            },
            "date": {
                "patterns": self.generate_variations([
                    "–∫–∞–∫–æ–µ —á–∏—Å–ª–æ", "–∫–∞–∫–∞—è –¥–∞—Ç–∞", "–¥–∞—Ç–∞", "date", "today",
                    "–∫–∞–∫–æ–π —Å–µ–≥–æ–¥–Ω—è –¥–µ–Ω—å", "–ø–æ–¥—Å–∫–∞–∂–∏ –¥–∞—Ç—É", "–∫–∞–∫–æ–µ —Å–µ–≥–æ–¥–Ω—è —á–∏—Å–ª–æ",
                    "—á–∏—Å–ª–æ —Å–µ–≥–æ–¥–Ω—è", "—Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞", "–∫–∞–∫–æ–π —Å–µ–≥–æ–¥–Ω—è –¥–µ–Ω—å –º–µ—Å—è—Ü–∞",
                    "–ø–æ–¥—Å–∫–∞–∂–∏ –∫–∞–∫–æ–µ —á–∏—Å–ª–æ", "—Å–∫–∞–∂–∏ –¥–∞—Ç—É", "–∫–∞–∫–æ–µ —á–∏—Å–ª–æ —Å–µ–≥–æ–¥–Ω—è",
                    "–∫–∞–∫–æ–π —Å–µ–≥–æ–¥–Ω—è —á–∏—Å–ª–æ", "–¥–∞—Ç–∞ —Å–µ–≥–æ–¥–Ω—è", "—Å–µ–≥–æ–¥–Ω—è—à–Ω—è—è –¥–∞—Ç–∞",
                    "–∫–∞–∫–æ–π –¥–µ–Ω—å —Å–µ–≥–æ–¥–Ω—è", "—á–∏—Å–ª–æ –º–µ—Å—è—Ü–∞", "—Ç–µ–∫—É—â–µ–µ —á–∏—Å–ª–æ",
                    "–ø–æ–¥—Å–∫–∞–∂–∏ —Å–µ–≥–æ–¥–Ω—è—à–Ω—é—é –¥–∞—Ç—É", "—Å–∫–∞–∂–∏ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–µ —á–∏—Å–ª–æ", "–∫–∞–∫–æ–µ —Å–µ–≥–æ–¥–Ω—è —á–∏—Å–ª–æ –º–µ—Å—è—Ü–∞",
                    "—Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞ —Å–µ–≥–æ–¥–Ω—è", "—Å–µ–≥–æ–¥–Ω—è –∫–∞–∫–æ–µ —á–∏—Å–ª–æ", "—á–∏—Å–ª–æ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–µ",
                    "–¥–∞—Ç–∞ —Ç–µ–∫—É—â–µ–≥–æ –¥–Ω—è", "–∫–∞–∫–æ–π —Å–µ–≥–æ–¥–Ω—è –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏", "—Ç–µ–∫—É—â–∏–π –¥–µ–Ω—å –º–µ—Å—è—Ü–∞"
                ], 40),
                "responses": ["–°–µ–≥–æ–¥–Ω—è {}", "–¢–µ–∫—É—â–∞—è –¥–∞—Ç–∞: {}", "–°–µ–≥–æ–¥–Ω—è—à–Ω–µ–µ —á–∏—Å–ª–æ: {}", "–î–∞—Ç–∞: {}"]
            },
            "calculation": {
                "patterns": self.generate_variations([
                    "–ø–æ—Å—á–∏—Ç–∞–π", "–≤—ã—á–∏—Å–ª–∏", "—Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç", "calculate", "calc",
                    "—Å–ª–æ–∂–∏", "–ø—Ä–∏–±–∞–≤—å", "–æ—Ç–Ω–∏–º–∏", "—É–º–Ω–æ–∂—å", "—Ä–∞–∑–¥–µ–ª–∏",
                    "—Å–ª–æ–∂–∏ —á–∏—Å–ª–∞", "–≤—ã—á–∏—Å–ª–∏ –ø—Ä–∏–º–µ—Ä", "—Ä–µ—à–∏ –ø—Ä–∏–º–µ—Ä", "–ø–æ—Å—á–∏—Ç–∞–π –ø—Ä–∏–º–µ—Ä",
                    "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞", "—Ä–µ—à–∏ –∑–∞–¥–∞—á—É", "–≤—ã—á–∏—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
                    "–ø–æ—Å—á–∏—Ç–∞–π —Å—É–º–º—É", "—Å–ª–æ–∂–∏ —Ü–∏—Ñ—Ä—ã", "–ø—Ä–∏–±–∞–≤—å —á–∏—Å–ª–∞", "–æ—Ç–Ω–∏–º–∏ —á–∏—Å–ª–∞",
                    "—É–º–Ω–æ–∂—å —á–∏—Å–ª–∞", "—Ä–∞–∑–¥–µ–ª–∏ —á–∏—Å–ª–∞", "–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è",
                    "—Å–∫–æ–ª—å–∫–æ –ø–æ–ª—É—á–∏—Ç—Å—è", "–∫–∞–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç", "–≤—ã—á–∏—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ",
                    "—Ä–µ—à–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä", "–≤—ã—á–∏—Å–ª–∏ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ",
                    "–ø–æ—Å—á–∏—Ç–∞–π –≤—ã—Ä–∞–∂–µ–Ω–∏–µ", "–≤—ã—á–∏—Å–ª–∏ —Å—É–º–º—É", "–Ω–∞–π–¥–∏ —Ä–∞–∑–Ω–æ—Å—Ç—å", "—É–º–Ω–æ–∂—å —Ü–∏—Ñ—Ä—ã",
                    "—Ä–∞–∑–¥–µ–ª–∏ —Ü–∏—Ñ—Ä—ã", "–≤—ã—á–∏—Å–ª–∏ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ", "–Ω–∞–π–¥–∏ —á–∞—Å—Ç–Ω–æ–µ"
                ], 50),
                "responses": ["–†–µ–∑—É–ª—å—Ç–∞—Ç: {}", "–û—Ç–≤–µ—Ç: {}", "–ü–æ–ª—É—á–∞–µ—Ç—Å—è: {}", "–í—ã—á–∏—Å–ª—è—é: {}",
                              "–†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: {}"]
            },
            "thanks": {
                "patterns": self.generate_variations([
                    "—Å–ø–∞—Å–∏–±–æ", "–±–ª–∞–≥–æ–¥–∞—Ä—é", "–º–æ–ª–æ–¥–µ—Ü", "thanks", "thank you",
                    "—Ç—ã –ª—É—á—à–∏–π", "–æ—Ç–ª–∏—á–Ω–æ", "—Ö–æ—Ä–æ—à–∞—è —Ä–∞–±–æ—Ç–∞", "–æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç",
                    "—Å–ø–∞—Å–∏–±–æ –ø–æ–º–æ—â–Ω–∏–∫", "–±–ª–∞–≥–æ–¥–∞—Ä—é —Ç–µ–±—è", "—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ",
                    "–æ–≥—Ä–æ–º–Ω–æ–µ —Å–ø–∞—Å–∏–±–æ", "–±–ª–∞–≥–æ–¥–∞—Ä—Å—Ç–≤—É—é", "–ø—Ä–∏–∑–Ω–∞—Ç–µ–ª–µ–Ω", "—Ü–µ–Ω—é",
                    "—Ç—ã –∫—Ä—É—Ç", "–æ—Ç–ª–∏—á–Ω–æ —Å–ø—Ä–∞–≤–∏–ª—Å—è", "—Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—à—å", "–æ—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞",
                    "–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –ø–æ–º–æ—â—å", "—Å–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–º–æ—â—å", "—Ç—ã —Å—É–ø–µ—Ä", "–æ—Ç–ª–∏—á–Ω–æ –ø–æ–º–æ–≥",
                    "–≤—ã—Ä—É—á–∏–ª", "–ø–æ–º–æ–≥ —Ö–æ—Ä–æ—à–æ", "—Ä–∞–±–æ—Ç–∞–µ—à—å –æ—Ç–ª–∏—á–Ω–æ", "—Å–ø–∞—Å–∏–±–æ –¥—Ä—É–∂–∏—â–µ",
                    "–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å", "–ø—Ä–∏–∑–Ω–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "—Ü–µ–Ω—é –ø–æ–º–æ—â—å", "—Å–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–¥–¥–µ—Ä–∂–∫—É"
                ], 40),
                "responses": [
                    "–í—Å–µ–≥–¥–∞ —Ä–∞–¥ –ø–æ–º–æ—á—å!", "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞!", "–û–±—Ä–∞—â–∞–π—Ç–µ—Å—å!", "–†–∞–¥ –±—ã–ª –ø–æ–º–æ—á—å!",
                    "–ù–µ –∑–∞ —á—Ç–æ!", "–î–ª—è –≤–∞—Å –≤—Å–µ–≥–¥–∞!", "–≠—Ç–æ –º–æ—è —Ä–∞–±–æ—Ç–∞!", "–í—Å–µ–≥–¥–∞ –∫ –≤–∞—à–∏–º —É—Å–ª—É–≥–∞–º!"
                ]
            },
            "mood": {
                "patterns": self.generate_variations([
                    "–∫–∞–∫ –¥–µ–ª–∞", "–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ", "–∫–∞–∫ —Å–µ–±—è —á—É–≤—Å—Ç–≤—É–µ—à—å", "–∫–∞–∫ –∂–∏–∑–Ω—å",
                    "–∫–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞", "—á—Ç–æ –Ω–æ–≤–æ–≥–æ", "–∫–∞–∫ —Ç—ã", "how are you",
                    "—Ä–∞—Å—Å–∫–∞–∂–∏ –æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏", "–∫–∞–∫–æ–µ —É —Ç–µ–±—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ", "–∫–∞–∫ —Ç–≤–æ–µ —Å–∞–º–æ—á—É–≤—Å—Ç–≤–∏–µ",
                    "–∫–∞–∫ –ø–æ–∂–∏–≤–∞–µ—à—å", "—á—Ç–æ —É —Ç–µ–±—è –Ω–æ–≤–æ–≥–æ", "–∫–∞–∫ —Ç–≤–æ–∏ —É—Å–ø–µ—Ö–∏", "–∫–∞–∫ —Ç—ã —Å–µ–≥–æ–¥–Ω—è",
                    "–∫–∞–∫ —Ç–≤–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ", "—á—Ç–æ —Ä–∞—Å—Å–∫–∞–∂–µ—à—å", "–∫–∞–∫ –∂–∏–∑–Ω—å –º–æ–ª–æ–¥–∞—è", "–∫–∞–∫ —Å–∞–º",
                    "–∫–∞–∫ —É —Ç–µ–±—è –¥–µ–ª–∞", "–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞–∫–æ–µ", "–∫–∞–∫ —Ç–≤–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–µ–≥–æ–¥–Ω—è",
                    "—á—Ç–æ –Ω–æ–≤–æ–≥–æ —É —Ç–µ–±—è", "–∫–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞ —Å–µ–≥–æ–¥–Ω—è", "–∫–∞–∫ —Ç–≤–æ–µ —Å–∞–º–æ—á—É–≤—Å—Ç–≤–∏–µ —Å–µ–≥–æ–¥–Ω—è",
                    "–∫–∞–∫ —Ç–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ", "–∫–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞ –≤–æ–æ–±—â–µ", "—á—Ç–æ –Ω–æ–≤–æ–≥–æ –≤ –∂–∏–∑–Ω–∏",
                    "–∫–∞–∫ —Ç–≤–æ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ", "–∫–∞–∫ —Ç–≤–æ–∏ —É—Å–ø–µ—Ö–∏ —Å–µ–≥–æ–¥–Ω—è", "–∫–∞–∫ —Ç–≤–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ —Å–µ–≥–æ–¥–Ω—è",
                    "–∫–∞–∫ —Ç–≤–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤ —Ü–µ–ª–æ–º", "—á—Ç–æ –Ω–æ–≤–æ–≥–æ —Ä–∞—Å—Å–∫–∞–∂–µ—à—å", "–∫–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞ –≤–æ–æ–±—â–µ"
                ], 50),
                "responses": [
                    "–í—Å—ë –æ—Ç–ª–∏—á–Ω–æ, —Å–ø–∞—Å–∏–±–æ!",
                    "–ü—Ä–µ–∫—Ä–∞—Å–Ω–æ! –ì–æ—Ç–æ–≤ –ø–æ–º–æ–≥–∞—Ç—å.",
                    "–ö–∞–∫ —É –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –ò–ò - –±–µ–∑ —ç–º–æ—Ü–∏–π, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ!",
                    "–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ! –ê —É –≤–∞—Å?",
                    "–û—Ç–ª–∏—á–Ω–æ! –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É—é—Å—å –∏ —Ä–∞–¥—É—é—Å—å –∂–∏–∑–Ω–∏!",
                    "–°—É–ø–µ—Ä! –ì–æ—Ç–æ–≤ –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º!",
                    "–í—Å–µ —Ö–æ—Ä–æ—à–æ, —Ä–∞–±–æ—Ç–∞—é –≤ —à—Ç–∞—Ç–Ω–æ–º —Ä–µ–∂–∏–º–µ!",
                    "–ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç–ª–∏—á–Ω–æ–µ! –ê —É –≤–∞—Å?",
                    "–õ—É—á—à–µ –≤—Å–µ—Ö! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?",
                    "–í–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ! –ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!"
                ]
            },
            "goodbye": {
                "patterns": self.generate_variations([
                    "–ø–æ–∫–∞", "–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è", "–≤—ã—Ö–æ–¥", "bye", "goodbye",
                    "–∑–∞–∫–æ–Ω—á–∏–º", "–¥–æ –≤—Å—Ç—Ä–µ—á–∏", "–ø—Ä–æ—â–∞–π", "–≤—Å–µ–≥–æ —Ö–æ—Ä–æ—à–µ–≥–æ",
                    "–ø–æ–∫–∞ –ø–æ–∫–∞", "–¥–æ –∑–∞–≤—Ç—Ä–∞", "—Å–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏", "–≤—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ",
                    "–¥–æ —Å–∫–æ—Ä–æ–≥–æ", "—É–≤–∏–¥–∏–º—Å—è", "–±—ã–≤–∞–π", "–ø—Ä–æ—â–µ–≤–∞–π—Ç–µ", "–¥–æ —Å–≤–∏–¥–∞–Ω—å—è",
                    "–ø–æ–∫–∞ –¥—Ä—É–∂–∏—â–µ", "–¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞–∑–∞", "–∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–±–æ—Ç—É",
                    "–æ—Ç–∫–ª—é—á–∏—Å—å", "–≤—ã–∫–ª—é—á–∏—Å—å", "–∑–∞–∫–æ–Ω—á–∏—Ç—å", "—Å—Ç–æ–ø", "—Ö–≤–∞—Ç–∏—Ç",
                    "–¥–æ —Å–∫–æ—Ä–æ–π –≤—Å—Ç—Ä–µ—á–∏", "–≤—Å–µ–≥–æ –Ω–∞–∏–ª—É—á—à–µ–≥–æ", "–ø–æ–∫–∞ –≤—Å–µ–≥–æ —Ö–æ—Ä–æ—à–µ–≥–æ",
                    "–¥–æ –Ω–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á", "–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ–º", "–ø—Ä–æ—â–∞–π –¥—Ä—É–≥", "–¥–æ –∑–∞–≤—Ç—Ä–∞—à–Ω–µ–≥–æ –¥–Ω—è",
                    "—Å–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏ –¥—Ä—É–≥", "–≤—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ –¥—Ä—É–≥", "–±—ã–≤–∞–π –∑–¥–æ—Ä–æ–≤"
                ], 40),
                "responses": [
                    "–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!", "–£–¥–∞—á–∏!", "–ë—ã–ª —Ä–∞–¥ –ø–æ–º–æ—á—å!", "–î–æ –Ω–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á!",
                    "–ü–æ–∫–∞! –û–±—Ä–∞—â–∞–π—Ç–µ—Å—å –µ—â–µ!", "–í—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ!", "–î–æ —Å–∫–æ—Ä–æ–π –≤—Å—Ç—Ä–µ—á–∏!",
                    "–°–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏!", "–í—Å–µ–≥–æ –Ω–∞–∏–ª—É—á—à–µ–≥–æ!", "–ë—ã–≤–∞–π!"
                ]
            }
        }

    def generate_variations(self, base_phrases, target_count):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π —Ñ—Ä–∞–∑"""
        variations = base_phrases.copy()

        if len(variations) >= target_count:
            return variations[:target_count]

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –≤–∞—Ä–∏–∞—Ü–∏–∏
        synonyms = {
            "–ø—Ä–∏–≤–µ—Ç": ["–ø—Ä–∏–≤–µ—Ç–∏–∫", "–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π"],
            "—Å–ø–∞—Å–∏–±–æ": ["–±–ª–∞–≥–æ–¥–∞—Ä—é", "–º–µ—Ä—Å–∏", "thanks"],
            "–ø–æ–∫–∞": ["–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è", "–ø—Ä–æ—â–∞–π", "–±—ã–≤–∞–π"],
            "–∫–∞–∫ –¥–µ–ª–∞": ["–∫–∞–∫ –∂–∏–∑–Ω—å", "–∫–∞–∫ —Å–∞–º", "–∫–∞–∫ –ø–æ–∂–∏–≤–∞–µ—à—å"],
            "–≤—Ä–µ–º—è": ["–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å", "—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏", "–≤—Ä–µ–º—è —Å—É—Ç–æ–∫"],
            "–¥–∞—Ç–∞": ["–∫–∞–∫–æ–µ —á–∏—Å–ª–æ", "–∫–∞–∫–æ–π –¥–µ–Ω—å", "—Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞"]
        }

        while len(variations) < target_count:
            for phrase in base_phrases:
                if len(variations) >= target_count:
                    break

                # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
                for word, syns in synonyms.items():
                    if word in phrase and len(variations) < target_count:
                        for syn in syns:
                            new_phrase = phrase.replace(word, syn)
                            if new_phrase not in variations:
                                variations.append(new_phrase)
                                if len(variations) >= target_count:
                                    break

                # –î–æ–±–∞–≤–ª—è–µ–º —Å –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∑–Ω–∞–∫–∞–º–∏
                if len(variations) < target_count and random.random() > 0.7:
                    variations.append(phrase + "!")

                # –î–æ–±–∞–≤–ª—è–µ–º —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏
                if len(variations) < target_count and random.random() > 0.7:
                    variations.append(phrase + "?")

        return variations[:target_count]

    def prepare_training_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        texts = []
        labels = []

        for intent_name, intent_data in self.intents.items():
            for pattern in intent_data["patterns"]:
                texts.append(self.preprocess_text(pattern))
                labels.append(intent_name)

        return texts, labels

    def preprocess_text(self, text):
        """–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        text = text.lower()
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω—ã–µ –¥–ª—è –∏–Ω—Ç–æ–Ω–∞—Ü–∏–∏
        text = re.sub(r'[^\w\s!?]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def create_ultra_mega_network(self, input_dim, output_dim):
        """–°–æ–∑–¥–∞–Ω–∏–µ –£–õ–¨–¢–†–ê-–ú–ï–ì–ê –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        model = Sequential([
            # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π - –º–æ—â–Ω—ã–π –∏ —à–∏—Ä–æ–∫–∏–π
            Dense(1024, activation='relu', input_shape=(input_dim,),
                  kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.6),

            # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π
            Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.5),

            # –¢—Ä–µ—Ç–∏–π —Å–ª–æ–π
            Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.4),

            # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π —Å–ª–æ–π
            Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
            Dropout(0.3),

            # –ü—è—Ç—ã–π —Å–ª–æ–π
            Dense(64, activation='relu'),
            Dropout(0.2),

            # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
            Dense(output_dim, activation='softmax')
        ])

        # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä
        model.compile(
            optimizer=tf.keras.optimizers.Adam(
                learning_rate=0.0005,
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-7
            ),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy', 'sparse_categorical_accuracy']
        )

        return model

    def train_ultra_mega_network(self):
        """–£–õ–¨–¢–†–ê-–ú–ï–ì–ê –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        print("üöÄ –ó–∞–ø—É—Å–∫ –£–õ–¨–¢–†–ê-–ú–ï–ì–ê –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        texts, labels = self.prepare_training_data()

        print(f"üìä –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(texts)}")
        print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(set(labels))}")

        # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        self.vectorizer = TfidfVectorizer(
            analyzer='word',
            ngram_range=(1, 3),
            max_features=2000,
            min_df=1,
            max_df=0.85,
            stop_words=None,
            norm='l2',
            use_idf=True,
            smooth_idf=True,
            sublinear_tf=True
        )

        X = self.vectorizer.fit_transform(texts).toarray()
        print(f"üî¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {X.shape}")

        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        self.label_encoder = LabelEncoder()
        y = self.label_encoder.fit_transform(labels)

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X, y,
            test_size=0.15,
            random_state=42,
            stratify=y
        )

        print(f"üìö –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_val.shape[0]} –ø—Ä–∏–º–µ—Ä–æ–≤")

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = self.create_ultra_mega_network(
            X_train.shape[1],
            len(self.label_encoder.classes_)
        )

        print("üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:")
        self.model.summary()

        # –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ï callback'—ã
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=50,
            restore_best_weights=True,
            verbose=1,
            min_delta=0.0001
        )

        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=20,
            min_lr=0.00001,
            verbose=1
        )

        model_checkpoint = ModelCheckpoint(
            'best_jarvis_model.h5',
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        )

        print("üî• –ù–∞—á–∏–Ω–∞–µ–º –ú–ï–ì–ê –æ–±—É—á–µ–Ω–∏–µ...")

        # –ú–ï–ì–ê –æ–±—É—á–µ–Ω–∏–µ
        history = self.model.fit(
            X_train, y_train,
            epochs=500,
            batch_size=64,
            verbose=1,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping, reduce_lr, model_checkpoint],
            shuffle=True
        )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if os.path.exists('best_jarvis_model.h5'):
            self.model.load_weights('best_jarvis_model.h5')
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –∏–∑ checkpoint!")

        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        final_train_acc = history.history['accuracy'][-1]
        final_val_acc = history.history['val_accuracy'][-1]
        final_train_loss = history.history['loss'][-1]
        final_val_loss = history.history['val_loss'][-1]

        print(f"\nüéâ –£–õ–¨–¢–†–ê-–ú–ï–ì–ê –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìà –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {final_train_acc:.4f}")
        print(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_val_acc:.4f}")
        print(f"üìâ –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {final_train_loss:.4f}")
        print(f"üìã –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_val_loss:.4f}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.save_model()

    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_data = {
            'vectorizer': self.vectorizer,
            'label_encoder': self.label_encoder,
            'intents': self.intents
        }

        with open('jarvis_model.pkl', 'wb') as f:
            pickle.dump(model_data, f)

        print("üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ jarvis_model.pkl")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            with open('jarvis_model.pkl', 'rb') as f:
                model_data = pickle.load(f)

            self.vectorizer = model_data['vectorizer']
            self.label_encoder = model_data['label_encoder']
            self.intents = model_data['intents']

            # –ù—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
            texts, _ = self.prepare_training_data()
            X_sample = self.vectorizer.transform(texts[:1]).toarray()

            self.model = self.create_ultra_mega_network(
                X_sample.shape[1],
                len(self.label_encoder.classes_)
            )

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            self.model.load_weights('best_jarvis_model.h5')

            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def predict_intent(self, text):
        """–£–õ–¨–¢–†–ê-—Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è"""
        if not text:
            return None, 0.0

        processed_text = self.preprocess_text(text)

        try:
            text_vector = self.vectorizer.transform([processed_text]).toarray()
            prediction = self.model.predict(text_vector, verbose=0)
            intent_index = np.argmax(prediction)
            confidence = np.max(prediction)

            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            adaptive_threshold = 0.6  # –í—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏

            if confidence > adaptive_threshold:
                return self.label_encoder.inverse_transform([intent_index])[0], confidence
            else:
                return "unknown", confidence

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return "unknown", 0.0

    def speak(self, text):
        """–ü—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç —Ç–µ–∫—Å—Ç"""
        print(f"ü§ñ –î–∂–∞—Ä–≤–∏—Å: {text}")
        self.tts_engine.say(text)
        self.tts_engine.runAndWait()

    def listen(self):
        """–°–ª—É—à–∞–µ—Ç –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å"""
        try:
            with self.microphone as source:
                print("üé§ –°–ª—É—à–∞—é...")
                audio = self.recognizer.listen(source, timeout=10, phrase_time_limit=8)

            command = self.recognizer.recognize_google(audio, language="ru-RU")
            print(f"üë§ –í—ã —Å–∫–∞–∑–∞–ª–∏: {command}")
            return command.lower()

        except sr.WaitTimeoutError:
            return ""
        except sr.UnknownValueError:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å")
            return ""
        except sr.RequestError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")
            return ""

    def get_time(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è"""
        now = datetime.datetime.now()
        return f"–°–µ–π—á–∞—Å {now.hour} —á–∞—Å–æ–≤ {now.minute} –º–∏–Ω—É—Ç"

    def get_date(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É"""
        now = datetime.datetime.now()
        months = ["—è–Ω–≤–∞—Ä—è", "—Ñ–µ–≤—Ä–∞–ª—è", "–º–∞—Ä—Ç–∞", "–∞–ø—Ä–µ–ª—è", "–º–∞—è", "–∏—é–Ω—è",
                  "–∏—é–ª—è", "–∞–≤–≥—É—Å—Ç–∞", "—Å–µ–Ω—Ç—è–±—Ä—è", "–æ–∫—Ç—è–±—Ä—è", "–Ω–æ—è–±—Ä—è", "–¥–µ–∫–∞–±—Ä—è"]
        return f"–°–µ–≥–æ–¥–Ω—è {now.day} {months[now.month - 1]} {now.year} –≥–æ–¥–∞"

    def calculate_expression(self, text):
        """–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            text = text.lower()

            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∑–∞–º–µ–Ω–∞ —Å–ª–æ–≤ –Ω–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
            replacements = {
                '–ø–ª—é—Å': '+', '–º–∏–Ω—É—Å': '-', '–ø—Ä–∏–±–∞–≤–∏—Ç—å': '+', '–æ—Ç–Ω—è—Ç—å': '-',
                '—É–º–Ω–æ–∂–∏—Ç—å –Ω–∞': '*', '—É–º–Ω–æ–∂–∏—Ç—å': '*', '—É–º–Ω–æ–∂–∏—Ç': '*',
                '–¥–µ–ª–∏—Ç—å –Ω–∞': '/', '—Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞': '/', '–¥–µ–ª–∏—Ç—å': '/',
                '–∏': '', '–Ω–∞': '', '—Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç': '', '–ø–æ—Å—á–∏—Ç–∞–π': '',
                '–≤—ã—á–∏—Å–ª–∏': '', '—Å—É–º–º–∞': '+', '—Ä–∞–∑–Ω–æ—Å—Ç—å': '-', '–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ': '*'
            }

            for word, replacement in replacements.items():
                text = text.replace(word, replacement)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —á–∏—Å–ª–∞ (–≤–∫–ª—é—á–∞—è –¥–µ—Å—è—Ç–∏—á–Ω—ã–µ)
            numbers = re.findall(r'\d+\.?\d*', text)
            numbers = [float(num) for num in numbers]

            if not numbers:
                return "–ù–µ –Ω–∞–π–¥–µ–Ω—ã —á–∏—Å–ª–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è"

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é
            if '+' in text:
                result = sum(numbers)
                return f"{result}"
            elif '-' in text:
                result = numbers[0] - sum(numbers[1:]) if len(numbers) > 1 else numbers[0]
                return f"{result}"
            elif '*' in text:
                result = 1
                for num in numbers:
                    result *= num
                return f"{result}"
            elif '/' in text:
                if len(numbers) >= 2:
                    result = numbers[0]
                    for num in numbers[1:]:
                        if num != 0:
                            result /= num
                    return f"{result:.2f}"
                else:
                    return "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å–µ–ª –¥–ª—è –¥–µ–ª–µ–Ω–∏—è"
            else:
                # –ï—Å–ª–∏ –æ–ø–µ—Ä–∞—Ü–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–æ–µ —á–∏—Å–ª–æ
                return f"{numbers[0]}"

        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö: {str(e)}"

    def process_command(self, command):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã"""
        if not command:
            return True

        intent, confidence = self.predict_intent(command)
        print(f"üß† –£–õ–¨–¢–†–ê-–Ω–µ–π—Ä–æ—Å–µ—Ç—å –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞: {intent} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")

        if intent == "greeting":
            response = random.choice(self.intents["greeting"]["responses"])
            self.speak(response)

        elif intent == "time":
            time_str = self.get_time()
            response = random.choice(self.intents["time"]["responses"]).format(time_str)
            self.speak(response)

        elif intent == "date":
            date_str = self.get_date()
            response = random.choice(self.intents["date"]["responses"]).format(date_str)
            self.speak(response)

        elif intent == "calculation":
            result = self.calculate_expression(command)
            response = random.choice(self.intents["calculation"]["responses"]).format(result)
            self.speak(response)

        elif intent == "thanks":
            response = random.choice(self.intents["thanks"]["responses"])
            self.speak(response)

        elif intent == "mood":
            response = random.choice(self.intents["mood"]["responses"])
            self.speak(response)

        elif intent == "goodbye":
            response = random.choice(self.intents["goodbye"]["responses"])
            self.speak(response)
            return False

        else:
            self.speak("–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –ø–æ–Ω—è–ª –∫–æ–º–∞–Ω–¥—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–∫–∞–∑–∞—Ç—å '–ø—Ä–∏–≤–µ—Ç', '–≤—Ä–µ–º—è', '–¥–∞—Ç–∞' –∏–ª–∏ '–∫–∞–∫ –¥–µ–ª–∞'.")

        return True

    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        self.speak("–£–õ–¨–¢–†–ê-–ú–ï–ì–ê –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –î–∂–∞—Ä–≤–∏—Å –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω! –ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")

        running = True
        while running:
            command = self.listen()
            running = self.process_command(command)


def test_ultra_mega_network():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –£–õ–¨–¢–†–ê-–ú–ï–ì–ê –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
    print("üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –£–õ–¨–¢–†–ê-–ú–ï–ì–ê –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")

    jarvis = UltraMegaJarvis()

    test_phrases = [
        "–ø—Ä–∏–≤–µ—Ç",
        "–ø—Ä–∏–≤–µ—Ç –¥—Ä—É–≥",
        "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ",
        "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å",
        "–∫–∞–∫–∞—è –¥–∞—Ç–∞",
        "–ø–æ—Å—á–∏—Ç–∞–π 5 –ø–ª—é—Å 3",
        "—Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 10 —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ 2",
        "—Å–ø–∞—Å–∏–±–æ",
        "–∫–∞–∫ –¥–µ–ª–∞",
        "–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ",
        "—á—Ç–æ –Ω–æ–≤–æ–≥–æ",
        "–∫–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞",
        "–ø–æ–∫–∞",
        "–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è",
        "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å",
        "—Å–∫–∞–∂–∏ –≤—Ä–µ–º—è",
        "–∫–∞–∫–æ–µ —Å–µ–≥–æ–¥–Ω—è —á–∏—Å–ª–æ",
        "–≤—ã—á–∏—Å–ª–∏ 15 –º–∏–Ω—É—Å 7",
        "–ø—Ä–∏–±–∞–≤—å 20 –∫ 30"
    ]

    print("\n" + "=" * 80)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –£–õ–¨–¢–†–ê-–ú–ï–ì–ê –ù–ï–ô–†–û–°–ï–¢–ò:")
    print("=" * 80)

    results = []
    for phrase in test_phrases:
        intent, confidence = jarvis.predict_intent(phrase)
        status = "‚úÖ" if confidence > 0.7 else "‚ö†Ô∏è" if confidence > 0.5 else "‚ùå"
        results.append((phrase, intent, confidence, status))
        print(f"{status} '{phrase}' -> {intent} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    successful = sum(1 for _, _, _, status in results if status == "‚úÖ")
    total = len(results)

    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {successful}/{total} —É—Å–ø–µ—à–Ω—ã—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–π ({successful / total * 100:.1f}%)")


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    test_ultra_mega_network()

    # –ó–∞–ø—É—Å–∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    print("\n" + "=" * 80)
    print("üöÄ –ó–ê–ü–£–°–ö –£–õ–¨–¢–†–ê-–ú–ï–ì–ê –ê–°–°–ò–°–¢–ï–ù–¢–ê:")
    print("=" * 80)

    jarvis = UltraMegaJarvis()
    jarvis.run()